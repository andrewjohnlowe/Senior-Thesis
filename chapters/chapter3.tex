%\begin{savequote}[75mm] 
%Nulla facilisi. In vel sem. Morbi id urna in diam dignissim feugiat. Proin molestie tortor eu velit. Aliquam erat volutpat. Nullam ultrices, diam tempus vulputate egestas, eros pede varius leo.
%\qauthor{Quoteauthor Lastname} 
%\end{savequote}

\chapter{Deep Learning -- Tying Unsupervised and Supervised Learning Together}

\newthought{Formalized, multi-level learning} is the main goal in deep learning -- developing a layered model corresponding to distinct levels of concepts. Deep architectures are composed of multiple levels of non-linear operations, such as in neural nets with many hidden layers or in complicated propositional formulae re-using many sub-formulae\citep{Bengio2009LearningDeep}.

\section{Biological Inspiration}

Trends in Neural Network formalism tend to coincide with changes in our understanding of how the brain works. An \ann can be formulated as clusters of neurons which fire via activation function

In order to thoroughly understand the notion of \dl{} within an \ann{}, I give a linear algebraic formulation, along with a formal description of training.

\section{A General Neural Network}

More often than not, a $k$-layered\footnote{I consider the input a layer.} \ann{} is thought of as a directed, acyclic $k$-partite graph, with some operator taking a transformed sum of a flow into each node. However, this is not a pragmatic way of formulating a neural network. In lieu of graphs, I use an  linear algebraic formulation. 
%\footnote{I consider the case where our targets are $k$ classes, encoded with 1-versus-all encoding, and we try to classify a vector \mathbf{x} as it is presented}

Formally, consider a dataset $X\in\R^{n\times p}$, where we have $p$ observations. Each observation is a vector $\mathbf{x}\in\R^n$. We also have a set of target values $T\in\R^{k\times p}$, where a target associated with an observation is a vector\footnote{Note that we are predicting $k$ targets.} $\mathbf{t}\in\R^k$.

An \ann{}, given an observation $\mathbf{x}$, tries to predict $\mathbf{t}\in\R^k$. Formally, a neural network $\mathcal{N}$ can be considered as a tuple $\{ C, f, g \}$, where:
\label{nn_conditions}
\begin{enumerate}

\item $f$ is some chosen neural activation function.
\item $g$ is either the identity function or the $\smax(\cdot)$ function.
\item $C$ is a collection of pairs of matrices and vectors ${(A_i, b_i)}_{i=1}^{n}$ such that \eqref{condition on dimensions} and \eqref{condition on product} are satisfied, and $\boxtimes_{C}^{f}(\mathbf{x})\in\R^k$
\end{enumerate}

Given this tuple, the \emph{hypothesis} produced from a neural network $\mathcal{N}$ can be written as

\begin{equation}
\label{net_prediction}
H(\mathbf{x} | \mathcal{N}) = g(\boxtimes_{C}^{f}(\mathbf{x}))
\end{equation}

which leads to a general, rather abstract formulation of the solution of the \ann{}. Let $E : (\mathbf{x}, \mathbf{y}) \longrightarrow \R$ be an error function that maps two vectors in a space to some value\footnote{Common choices include Sum-of-Squared-Errors and Cross-Entropy}. We try to find

\begin{equation}
\label{prob_formulation}
\min_{C} \sum_{(\mathbf{x}, \mathbf{t}) \in (X, T)}E(\mathbf{t}, H(\mathbf{x} | \mathcal{N}))
\end{equation}

which solves our problem of the best neural network given our observed $X$ and $T$. However, this is a purely theoretical formulation -- we must place constraints on how to solve the minimization problem outlined in \eqref{prob_formulation}.


\section{Pragmatism in forming a hypothesis}

How do we choose an activation function, as specified in the conditions for the neural network in \ref{nn_conditions} on page \pageref{nn_conditions}?

Clearly, the formalized prediction from the \nn{} \`{a} la \eqref{net_prediction} is a series of simple \texttt{AXPY} operations which can be easily optimized. Take our \nn{} $\mathcal{N}=\{ C, f, g \}$. From $$H(\mathbf{x} | \mathcal{N}) = g(\boxtimes_{C}^{f}(\mathbf{x}))$$, we can see the following. Suppose we have a \nn{} with $k$ layers. We will then have $k-1$ matrices and vectors in $C$. Label them $(W_i, b_i)$. We let $\mathbf{a}^{(1)} = \mathbf{x}$,  which allows us to say that 
\begin{eqnarray}
\mathbf{z}^{(i)} &=& W_i \mathbf{a}^{(i)} + b_i\\
\mathbf{a}^{(i+1)} &=& 
	\begin{cases} 
		f(\mathbf{z}^{(i)}), & \mbox{if } i+1 \neq k \\ 
		g(\mathbf{z}^{(i)}), & \mbox{otherwise}
	\end{cases}
\end{eqnarray}

Such a formulation lends itself to implementation in any language that can accommodate fast linear algebra.

\section{How can we train a basic \ann{}?}
\label{basix}

Training an \ann{} is done most frequently by so-called \emph{backpropagation}. Backpropagation proves itself useful in the training of both basic and Deep \nn{}s. In this section, we discuss -- in detail -- the implementation of backpropagation using efficient linear algebra.

We can break backpropagation into two parts --  the \emph{feed-forward step} step and \emph{backpropagation of errors}. Essentially, we use the structure of the neural network to define a gradient of cost function associated with the prediction errors that the network produces. 

More formally, suppose we have a collection of data points (column vectors) $\{\mathbf{x}_i\}_{i=1}^n$, where we have that $\mathbf{x}_i\in\Rn$. 


\section{Extracting Features using \ann{}s}

\subsection{The Basic Autoencoder}

We now shift our attention to \ann{}s that serve as unsupervised learning algorithms. Using \ann{}s, we can learn a new representation of our original data in a new feature space. The process  consists of two steps -- an encoding step, and a decoding step. Such \ann{} is called an \emph{autencoder}. We learn a non-linear \emph{encoding} that can then be linearly \emph{decoded} to recover the original dataset. Obtaining a reliable non-linearly extracted lower dimensional representation is extremely valuable. Intuitively, we train our neural network to estimate an approximation of the identity function. We now provide a simple formulation of such an idea.

Formally, consider the following. Consider our \emph{unlabelled} dataset$\{\mathbf{x}_i\}_{i=1}^n$, where we have that $\mathbf{x}_i\in\R^k$. Suppose we want to map each $\mathbf{x}_i$ to a new vector $\mathbf{y}\in\R^p$ with $p < k$. We wish to find two matrices -- $A\in\R^{p\times k}, B\in\R^{k\times p}$ -- and two vectors  $\mathbf{a}\in\R^p, \mathbf{b}\in\R^k$, that form good encoder-decoder pair as follows. For any $i$, the encoded features of $\mathbf{x}_i$ are represented by the vector 
\begin{equation}
\mathbf{f}_i = s(A\mathbf{x}_i + a)
\end{equation} 
which is clearly in $\R^p$. We now have that our reconstructed input is 
\begin{equation}
\tilde{\mathbf{x}}_i = B\mathbf{f}_i+b
\end{equation}

To solve this problem, i.e., to have $\tilde{\mathbf{x}}_i$ be similar to $\mathbf{x}_i$, we formulate this as an optimization problem. Our problem now becomes
\begin{equation}
\label{encodeobj}
\min_{A, B, \mathbf{a},\mathbf{b}} \frac{1}{2}\sumn{i} \Vert \mathbf{x}_i - \tilde{\mathbf{x}}_i \Vert ^ 2
\end{equation}

Clearly, we cannot find an analytical solution to this problem -- however, we can use gradient descent to to find our encoder pair $(A,\mathbf{a})$ and decoder $(B, \mathbf{b})$. Similarly to Section \ref{basix}, we can derive gradients of Equation \eqref{encodeobj}. We can re-write Equation \eqref{encodeobj} as 

\begin{equation}
\min_{A,B,a,b} \rho(A,B,a,b)
\end{equation}

where 
\begin{equation}
\label{eqrho}
\rho(A,B,a,b) = \frac{1}{2}\sumn{i} \Vert \mathbf{x}_i -  (B [s(A\mathbf{x}_i + a)]+b) \Vert ^ 2
\end{equation}

By taking matrix derivatives, we then have that, for a given observation $\mathbf{x}_i$,

\begin{eqnarray}
\label{grads_of_auto}
\Deriv{\rho}{B}&=& -(\mathbf{x}_i -\tilde{\mathbf{x}}_i) \mathbf{f}_i^T \\ 
\Deriv{\rho}{b}&=& -(\mathbf{x}_i - \tilde{\mathbf{x}}_i) \\
\Deriv{\rho}{A}&=& -B^T(\mathbf{x}_i -\tilde{\mathbf{x}}_i) \odot s'(\mathbf{f}_i)\mathbf{x}_i^T \\
\Deriv{\rho}{a}&=& -B^T(\mathbf{x}_i -\tilde{\mathbf{x}}_i) \odot s'(\mathbf{f}_i)
\end{eqnarray}

which allows us to use a gradient update step of
\begin{equation}
\theta \longleftarrow \theta - \gamma \Deriv{\rho}{\theta}
\end{equation}
for each matrix $\theta\in\{A, B, a, b\}$ and some small learning rate $\gamma$.

\subsection{Stacking and Robust Knowledge}
The basic Autoencoder performs well, but can be improved upon. It is desirable to have such an encoder-decoder pair to be resistant to inherent noise in the training data. Thus, in practice, we perturb the inputs to the Autoencoder, and attempt to have the reconstruction reproduce a original, un-noisy version from the perturbed observation. Formally, for any vector $\textbf{x}\in\R^{k}$ we present, let $\delta\mathbf{x}\sim N(\mathbf{0}_k, \varepsilon^2 I)$, where $\mathbf{0}_k$ is the $k$ dimensional vector of zeros, and $\varepsilon$ is some adequately small scalar value. Now, our reconstructed vector of $\tilde{\mathbf{x}}$ of our input $\mathbf{x}$ will be 

\begin{equation}
\label{denoising}
\tilde{\mathbf{x}} = B[s(A(\mathbf{x} + \delta\mathbf{x}) + \mathbf{a})] + \mathbf{b}
\end{equation}

In this case the gradient update steps in \ref{grads_of_auto} do not change. We still find the gradient with respect to the error when compared to the unperturbed observation. This process, though though not provable, creates a resistance to noise, making the features learned by the autoencoder more resistant to noise. Thus, we refer to equation \eqref{denoising} as a Denoising Autoencoder. As new, previously unseen examples are presented, we assume that they have noise associated with them, and are presented as-is --i.e., no perturbation -- to the encoder. In addition to creating more robust features, such a procedure makes the risk of overtraining much lower.

Take any trained denoising autoencoder with parameters $(A,B,a,b)$ that maps vectors $\mathbf{x}\in\R^k$ to features $\mathbf{f}\in\R^{p}$, with $p<k$. Define the function

\begin{equation}
\label{featuremapping}
\mathcal{F}_{(A,a)}^{k\rightarrow p}(\mathbf{x}) = s(A(\mathbf{x}) + \mathbf{a})
\end{equation}

which is syntactical sugar around the encoding step, which involves the encoding pair $(A,a)$. Suppose we wished to develop more abstract features. Using equation \eqref{featuremapping}, we can develop a collection of increasing \emph{abstract} autoencoders -- that is, autoencoders that develop progressively more abstract representations of the dataset. 

Formally, suppose we start from a dataset of dimension $p = d_0$, and we wish to map -- iteratively --  to each $\d_i\in\{d_i\}_{i=1}^{\ell}$, that is we map to $d_1$ dimensions, then to $d_2$ dimensions, et cetera. In addition, suppose we have a training set $\{\mathbf{x}_j\}_{j=1}^n$. For each $i\in\{1,\ldots,\ell\}$, let


\begin{equation}
\begin{aligned}
\label{auto_soln}
& (A_i,B_i,a_i,b_i) = \underset{(A,B,a,b)}{\text{argmin}}
& & \frac{1}{2}\sumn{j} \Vert \mathbf{f}_j^{i-1} -  (B[s(A(\mathbf{f}_j^{i-1}+\delta\mathbf{f}_j^{i-1}) + a)]+b) \Vert ^ 2 \\
& \text{subject to}
& & A_i\in\R^{d_i\times d_{i-1}}, a_i\in\R^{d_{i-1}}, B_i\in\R^{d_{i-1}\times d_{i}}, a_i\in\R^{d_{i}} \\
& \text{where}
& & \mathbf{f}^{i}_j = 
	\begin{cases} 
		\mathcal{F}_{(A_{i},a_{i})}^{d_{i-1}\rightarrow d_i}(\mathbf{f}_j^{i-1}), & \mbox{if } i>0\\ 
		\mathbf{x}_j, & \mbox{otherwise}
	\end{cases}
\end{aligned}
\end{equation}


We now have that for each layer $i$, $(A_i,B_i,a_i,b_i)$ is a Denoising Autoencoder. Equation \eqref{auto_soln} provides us with a formalization of greedy, layer-wise training presented in \citep{Vincent:2010fk}. We can greedily start from the first layer, and then train each following layer on the features extracted from the previous layer.







\section{Deep Neural Network Architecture}

Theoretically, there is no reason to have an upper limit on the size of a \nn{}. Adding additional layers to \nn{}s broadens the classes of functions that are representable. However, in practice, we see a phenomenon called a \emph{vanishing gradient}. We (empirically) see that gradients in layers that are farther away from the target values tend towards zero \citep{trainingdifficulty}. This only becomes a problem when we consider how our network is initiated. Since we originally initiate all values in the network to some range $[-a,a]$, we run an increasing risk of any layer being trapped in a local minimum with smaller gradients. 

























